{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkConf, SparkContext\n",
    "# conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "# sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# def parseLine(line):\n",
    "#     fields=line.split(',')\n",
    "#     age=int(fields[2])\n",
    "#     numFriends=int(fields[3])\n",
    "#     return(age,numFriends)\n",
    "\n",
    "# lines=sc.textFile(\"/home/jovyan/work/Desktop/sandbox/SparkCourse/SparkCourse/fakefriends.csv\")\n",
    "# rdd=lines.map(parseLine)\n",
    "# totalsByAge=rdd.mapValues(lambda x: (x,1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1]+y[1]))\n",
    "# averagesByAge=totalsByAge.mapValues(lambda x:x[0]/x[1])\n",
    "\n",
    "results = averagesByAge.collect()\n",
    "# for item in results:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,Will,33,385\n",
      "1,Jean-Luc,26,2\n",
      "2,Hugh,55,221\n",
      "3,Deanna,40,465\n",
      "4,Quark,68,21\n",
      "5,Weyoun,59,318\n",
      "6,Gowron,37,220\n",
      "7,Will,54,307\n",
      "8,Jadzia,38,380\n",
      "9,Hugh,27,181\n",
      "10,Odo,53,191\n",
      "11,Ben,57,372\n",
      "12,Keiko,54,253\n",
      "13,Jean-Luc,56,444\n",
      "14,Hugh,43,49\n",
      "15,Rom,36,49\n",
      "16,Weyoun,22,323\n",
      "17,Odo,35,13\n",
      "18,Jean-Luc,45,455\n",
      "19,Geordi,60,246\n",
      "20,Odo,67,220\n",
      "21,Miles,19,26\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/jovyan/work/Desktop/sandbox/SparkCourse/SparkCourse/fakefriends.csv\", 'r') as fin:\n",
    "    print(fin.read(334))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField,IntegerType,StringType,FloatType \n",
    "sqlContext = SQLContext(sc)\n",
    "schema = StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('Date',IntegerType(),True),\n",
    "    StructField('Age',IntegerType(),True),\n",
    "    StructField('Num Friends',StringType(),True)\n",
    "])\n",
    "\n",
    "dff = sqlContext.createDataFrame(rdd,schema)\n",
    "\n",
    "dff.show()\n",
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Date: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Num Friends: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_l_output = line_lengths.reduce(line_lengths)\n",
    "# z= line_l_output.collect()\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feb 19th \n",
    "reformatted csv - trying to see output.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ITE00100554': '60', 'GM000010962': '0', 'EZE00100082': '58'}\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext \n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "sc=SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "\n",
    "def praseLine(line):\n",
    "    fields=line.split(\",\")\n",
    "    stationID=fields[0]\n",
    "    entryType=fields[2]\n",
    "    temperature= float(fields[3]) * 0.1 * (9.0/5.0) +32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "#header_lst = ['StationID','Date','Type', 'Value','Un1','Un2','Un3','Un4']\n",
    "weather_lines = sc.textFile(\"/home/jovyan/work/Desktop/sandbox/SparkCourse/SparkCourse/1800_.csv\" )\n",
    "# parsedLines=weather_lines.map(parseLine)\n",
    "# minTemps=parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "# stationTemps = minTemps.map(lambda x: (x[0], x[2]) ) \n",
    "# minTemps = stationTemps.reduceByKey(lambda x,y: min(x,y))\n",
    "\n",
    "weatherz = {}\n",
    "for result in resultsz[:100]:\n",
    "    g= result.split(',')\n",
    "    location = g[0]\n",
    "    date = g[1]\n",
    "    val_type = g[2]\n",
    "    value = g[3]\n",
    "    \n",
    "    if location not in weatherz:\n",
    "        weatherz[location]=value\n",
    "    if location in weatherz and val_type=='TMAX':\n",
    "        weatherz[location]=value\n",
    "print(weatherz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "list of strings.\n",
    "Each string has a primary key location, a date, TMAX/TMIN/PRCP\n",
    "\n",
    "Check length of string, add value to dict based on length:\n",
    "for each location: \n",
    "\n",
    "\n",
    "dict = {'loc_ID':[TMAX,TMIN,PRCP,]}\n",
    "\n",
    "\n",
    "Cant seem to get\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf, SparkContext \n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc=SparkContext.getOrCreate(conf=conf)\n",
    "input = sc.textFile('/home/jovyan/work/Desktop/sandbox/SparkCourse/SparkCourse/book.txt')\n",
    "words = input.flatMap(lambda x: x.split())\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "#wordCounts_ = wordCounts.show()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|      Self|\n",
      "|Employment|\n",
      "|  Building|\n",
      "|        an|\n",
      "|  Internet|\n",
      "|  Business|\n",
      "|        of|\n",
      "|       One|\n",
      "| Achieving|\n",
      "| Financial|\n",
      "|       and|\n",
      "|  Personal|\n",
      "|   Freedom|\n",
      "|   through|\n",
      "|         a|\n",
      "| Lifestyle|\n",
      "|Technology|\n",
      "|  Business|\n",
      "|        By|\n",
      "|     Frank|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "video #28\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import functions as func \n",
    "\n",
    "spark = SparkSession.builder.appName('WrdCount').getOrCreate()\n",
    "inputDF = spark.read.text('/home/jovyan/work/Desktop/sandbox/SparkCourse/SparkCourse/book.txt')\n",
    "\n",
    "words = inputDF.select(func.explode(func.split(inputDF.value, \"\\\\W+\")).alias(\"word\"))\n",
    "words.collect()\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
